services:
  ollama:
    image: ollama/ollama:latest
    ports: ["11434:11434"]
    volumes:
      - ollama:/root/.ollama
    environment:
      - OLLAMA_KEEP_ALIVE=24h
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  redis:
    image: redis:7
    ports: ["6379:6379"]

  api:
    build: ./backend
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000
    ports: ["8000:8000"]
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - PYTHONPATH=/app
    volumes:
      - ./outputs:/outputs
      - ./fonts:/fonts
      - ./backend/worker.py:/worker.py
      - ./backend/app:/app/app
    depends_on:
      - redis
      - ollama
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]

  worker:
    build: ./backend
    command: rq worker -u redis://redis:6379 meme
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - PYTHONPATH=/app
    volumes:
      - ./outputs:/outputs
      - ./fonts:/fonts
      - ./backend:/app
    depends_on:
      - redis
      - ollama
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]

  video-worker:
    build: ./backend
    command: rq worker -u redis://redis:6379 video
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - PYTHONPATH=/app
    volumes:
      - ./outputs:/outputs
      - ./fonts:/fonts
      - ./backend:/app
    depends_on:
      - redis
      - ollama
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]

volumes:
  ollama:
